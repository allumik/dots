#!/bin/sh
#
# http://github.com/mitchweaver/bin
#
# play youtube subscriptions with dmenu, mpv, and youtube-dl
# modified to use my custom menu instead
#

# create dir, empty if already exists
dir=/tmp/yt
opml_file=${HOME}/var/cache/youtube_subs.opml
mkdir /tmp/yt 2> /dev/null
rm /tmp/yt/* 2> /dev/null

# parse the terrible xml that is our opml subscription file
# dump the channel names and their rss links to corresponding files
cat $opml_file | tr '><' '\n' | \
    sed -e 's/" title.*.xmlUrl="/\n/g' \
        -e 's/outline text="//g' \
        -e 's/" \///g' \
        -e '/^$/d' \
        -e '1,7d' |
    head -n -3 | \
    while IFS='' read -r line || [ -n "$line" ] ; do
        if echo $line | grep http > /dev/null ; then
            echo "$line" >> "$dir"/rss_links
        else
            echo "$line" >> "$dir"/chan_names
        fi
    done

# takes a channel rss feed link and creates files of titles, urls, dates, and channels
get_vids() {
    curl -s `cat /tmp/yt/rss_links | sed -n "$1p"` > "$dir"/dl$1 || return

    date=`cat "$dir"/dl$1 | grep '<published>' | sed -E -e 's/( )*<\/?published>//g' -e 's/\+.*//g' -e '1d'`
    if ! echo "$date" | grep `date "+%Y"` > /dev/null ; then
        return
    else
        echo "$date" >> "$dir"/vid_dates$1
    fi

    cat "$dir"/dl$1 | grep '<media:title>' | sed -E 's/( )*<\/?media:title>//g' >> "$dir"/vid_titles$1
    cat "$dir"/dl$1 | grep '<media:content url=' | sed -E -e 's/( )*<\/?media:content url="//g' -e 's/" type.*//g' >> "$dir"/vid_urls$1

    for i in $(seq 1 $(echo "$date" | wc -l)) ; do
        cat "$dir"/chan_names | sed -n "$1p" >> "$dir"/vid_chans$1
    done

    # cleanup
    rm "$dir"/dl$1
}

echo "1. fetching ..."
count=0
for i in `seq 1 $(cat "$dir"/rss_links | wc -l)` ; do
    get_vids $i &
    count=$(( $count + 1 ))
done

# wait for background processes to finish
wait

# concantenate all subfiles into collated files
for i in `seq 1 $(cat "$dir"/rss_links | wc -l)` ; do
    for file in vid_dates vid_chans vid_titles vid_urls ; do
        if [ -f "$dir"/"$file"$i ] ; then
            cat "$dir"/"$file"$i >> "$dir"/$file
            rm "$dir"/"$file"$i
        fi
    done
done

# concantenates files side by side, line by line
# note: paste can't use multi-char delimiters, and it cant do full unicode
#       using '`' as a junk character here so it can later be split into
#       my desired delimiter with a sed replace
echo "2. concatenating ..."
paste -d '`' "$dir"/vid_dates "$dir"/vid_chans "$dir"/vid_titles "$dir"/vid_urls > "$dir"/concat

# clean up any irregularities in the final file
# sed -E -i 's/( )?<\/?media:description>.//' $dir/concat

# now that we have all our information in the file, since we gathered them
# channel by channel we want to collate them based on date
echo "3. sorting ..."
sort -r "$dir"/concat -o "$dir"/concat

# send through our discovered info, nicely formatted, to dmenu
choice=`cat "$dir"/concat  | awk -F '\`' '{print $2 ": " $3}' | head -n 50 | menu sel -p 'Subscriptions:' | sed 's/: /\`/'`

[ -z "$choice" ] && exit

# read in the final concatenated file, and check for what our dmenu choice was
# grab the link, and run it with mpv
while IFS='' read -r line || [ -n "$line" ] ; do
        if echo $line | grep "$choice" > /dev/null ; then
            url=`echo $line | grep -oe 'http.*'`
        fi
done < /tmp/yt/concat

# make sure we didn't back out of dmenu before trying to run
[ -n "$url" ] && [ $(echo $url | wc -l) -eq 1 ] && \
    mpv "$(echo $url | sed -e 's/https:\/\//ytdl:\/\//')"
